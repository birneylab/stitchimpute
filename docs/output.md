# birneylab/stitchimpute: Output

<!--
# nf-core/stitchimpute: Output
-->

## Introduction

This document describes the output produced by the pipeline.

The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.

According to the pipeline workflow selected, the output folder with have different subfolders, which I indicate as `{group}` in the following explanation.
`{group}` will be nothing in the imputation workflow.
In the grid search workflow, it will be a string "K*{K}\_nGen*{nGen}" with the corresponding value of K and nGen for a given combination of parameters.
In the SNP set refinement workflow, it will be a string "iteration\_{n}" with the corresponding iteration number.

## Pipeline overview

The pipeline is built using [Nextflow](https://www.nextflow.io/) and processes data using the following steps:

- [Ground truth](#groundtruth) - Conversion of the ground truth to the anndata format and downsampling of high-coverage cram files
- [Stitch](#stitch) - Raw output from the STITCH imputation per chromosome
- [Joint output](#joint) - Full-genome imputation output
- [Performance](#performance) - Imputation performance per SNP according to different metrics
- [Pipeline information](#pipeline-information) - Report metrics generated during the workflow execution

### Ground truth

<details markdown="1">
<summary>Output files</summary>

- `ground_truth/`
  - `anndata/*.anndata.zarr`: Information from `ground_truth_vcf` in anndata zarr format for easier manipulation
  - `downsampled_reads/*.cram`: Downsampled version of the cram files used in the pipeline
  - `downsampled_reads/*.cram.crai`: Index files for the downsampled cram files

</details>

### Stitch

<details markdown="1">
<summary>Output files</summary>

- `{group}/stitch/chromosome_*`
  - `plots/`: Plots produced by STITCH
  - `RData/`: Intermediate STITCH results as R objects
  - `chromosome_*.vcf.gz`: Imputed VCF file for the chromosome
  - `chromosome_*.vcf.gz.csi`: Index file for the VCF

</details>

### Joint output

<details markdown="1">
<summary>Output files</summary>

- `{group}/joint_stitch_output`
  - `vcf/joint_stitch_output.vcf.gz`: Full genome imputed genotypes
  - `vcf/joint_stitch_output.vcf.gz.csi`: VCF index
  - `anndata/joint_stitch_output.anndata.zarr`: Full genome imputed genotypes in anndata format

</details>

### Performance

Imputation performance per SNP according to different metrics. The summary file has the following columns:

```
chr,pos,ref,alt,info_score,pearson_r
```

The `pearson_r` column is present only if `ground_truth_vcf` is set.

<details markdown="1">
<summary>Output files</summary>

- `{group}/performance_summaries`
  - `joint_stitch_output.performance.csv.gz`: CSV file containing the imputation performance results
- `imputation_quality_plots`
  - `*.pdf`: Plots produced in R with ggplot2 and cowplot showing the cumulative density of the different performance metrics, group by iteration/parameter combination in the respective workflows

</details>

### Pipeline information

<details markdown="1">
<summary>Output files</summary>

- `pipeline_info/`
  - Reports generated by Nextflow: `execution_report.html`, `execution_timeline.html`, `execution_trace.txt` and `pipeline_dag.dot`/`pipeline_dag.svg`.
  - Reports generated by the pipeline: `pipeline_report.html`, `pipeline_report.txt` and `software_versions.yml`. The `pipeline_report*` files will only be present if the `--email` / `--email_on_fail` parameter's are used when running the pipeline.
  - Reformatted samplesheet files used as input to the pipeline: `samplesheet.valid.csv`.

</details>

[Nextflow](https://www.nextflow.io/docs/latest/tracing.html) provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.

## Note on anndata, scikit-allel, and zarr

[Zarr](https://zarr.dev/) is a format for the storage of large multidimensional arrays.
Combined with [Dask](https://www.dask.org/), zarr allows to operate on larger-than-memory matrices.
[Anndata](https://anndata.readthedocs.io/en/latest/) is a Python package that provides an annotated data matrix. It can use zarr as a storage format and Dask for certain computations.
Internally the pipeline uses [scikit-allel](https://scikit-allel.readthedocs.io/en/stable/) to convert VCF files to the zarr format, and then with a custom script it converts the scikit-allel output to an anndata object that is serialised to the zarr format. This enormously simplifies data manipulations and minimises the chance for errors.

Since the zarr anndata objects are produced by the pipeline in any case, I also save them as outputs for further exploration and for interfacing with other pipelines (e.g. GWAS pipeline).
